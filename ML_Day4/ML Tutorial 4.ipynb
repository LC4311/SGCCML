{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can breakdown the problem of Machine Learning into two subtypes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, these are problems where we are trying to predict a continuous variable. There are a few models that you \n",
    "can use to solve these type of problems:\n",
    "\n",
    "- Ordinary Least Squares (Linear Regression)\n",
    "- LASSO Regression\n",
    "- Ridge Regression\n",
    "- k nearest neighbours (kNN) Regression\n",
    "- Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should already have seen and used the Linear Regression in practice. All subsequent models are similar in nature in the sense that we are trying to minimise the error function on the parameter space. I.e, find a set of $\\pmb{\\beta}$ such that $\\epsilon(\\pmb{\\beta}) \\geq 0$ is minimum. We saw that the error function for the **ordinary linear regression** is as follows: \n",
    "\n",
    "$$\\epsilon(\\pmb{\\beta}) = MSE(\\pmb{\\beta})$$\n",
    "\n",
    "For the LASSO, Ridge and Elastic Net regression models, we are trying to minimise the following functions with respect to $\\pmb{\\beta}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\epsilon(\\pmb{\\beta}) = MSE(\\pmb{\\beta}) + \\alpha\\sum_{i=1}^{n}|\\beta|_i$$ for some chosen $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\epsilon(\\pmb{\\beta}) = MSE(\\beta) + \\alpha\\pmb{\\beta}^\\intercal\\pmb{\\beta}$$ for some chosen $\\alpha$. This is also equivalent to:\n",
    "$$\\epsilon(\\pmb{\\beta}) = MSE(\\beta) + \\alpha\\sum_{i=1}^{n}\\beta_{i}^2$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are problems where we the target variable is **categorical**. The following models \n",
    "can be used to solve these type of problems:\n",
    "\n",
    "- Logistic Regression\n",
    "- kNN Classification\n",
    "- Decision Trees\n",
    "\n",
    "Note that the models listed so far are by no means exhaustive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling on Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
       "       'species'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement **one hot encoding** in our feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"is_setosa\"] = (iris['species'] == \"setosa\")*1\n",
    "iris[\"is_virginica\"] = (iris['species'] == \"virginica\")*1\n",
    "iris[\"is_versicolor\"] = (iris['species'] == \"versicolor\")*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species',\n",
       "       'is_setosa', 'is_virginica', 'is_versicolor'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the data-set into our test and training sets containing 30% and 70% data respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.int(0.3*iris.shape[0])\n",
    "idx = np.arange(0,iris.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "iris_test = iris.iloc[idx[:n]]\n",
    "iris_train = iris.iloc[idx[n:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to predict the sepal width of the plant based on the other features minus the species. Let's consider the ridge regression model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to standardise the feature observations as the $\\alpha$ weights each parameter equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, import the ridge regression model from the sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_model = Ridge(alpha=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started with data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first drop the target column from our feature set, as well as our species column, and convert the dataframe into a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iris_train.drop(['species','sepal_width'],axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(x)\n",
    "x = scaler.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = iris_train['sepal_width']\n",
    "\n",
    "ridge_model.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit the model using the predictors from the test set, be sure to scale your predictors first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = iris_test['sepal_width']\n",
    "predictor = iris_test.drop(['species','sepal_width'],axis=1).values\n",
    "scaler.fit(predictor)\n",
    "predictor = scaler.transform(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = ridge_model.predict(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09038565954093915"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE = ((obs - predicted)**2).mean()\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try testing the model on a different $\\alpha$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ridge(alpha):\n",
    "    ridge_model = Ridge(alpha)\n",
    "    x = iris_train.drop(['species','sepal_width'],axis = 1).values\n",
    "    scaler.fit(x)\n",
    "    x = scaler.transform(x)\n",
    "    y = iris_train['sepal_width']\n",
    "    ridge_model.fit(x,y)\n",
    "    obs = iris_test['sepal_width']\n",
    "    predictor = iris_test.drop(['species','sepal_width'],axis=1).values\n",
    "    scaler.fit(predictor)\n",
    "    predictor = scaler.transform(predictor)\n",
    "    predicted = ridge_model.predict(predictor)\n",
    "    MSE = ((obs - predicted)**2).mean()\n",
    "    return MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x113f119e8>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_vec = pd.Series(np.arange(0.001,100,1))\n",
    "mse_vec = alpha_vec.apply(train_ridge)\n",
    "plt.plot(mse_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training a LASSO model on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LASSO(alpha):\n",
    "    ridge_model = Lasso(alpha)\n",
    "    x = iris_train.drop(['species','sepal_width'],axis = 1).values\n",
    "    scaler.fit(x)\n",
    "    x = scaler.transform(x)\n",
    "    y = iris_train['sepal_width']\n",
    "    ridge_model.fit(x,y)\n",
    "    obs = iris_test['sepal_width']\n",
    "    predictor = iris_test.drop(['species','sepal_width'],axis=1).values\n",
    "    scaler.fit(predictor)\n",
    "    predictor = scaler.transform(predictor)\n",
    "    predicted = ridge_model.predict(predictor)\n",
    "    MSE = ((obs - predicted)**2).mean()\n",
    "    return MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x113fa22e8>]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_vec = pd.Series(np.arange(0.1,100,1))\n",
    "mse_vec = alpha_vec.apply(train_LASSO)\n",
    "plt.plot(mse_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection using Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of splitting the data into train and test sets extends to a routine where we can split the data up further so as to permutate what data the model can be used to train on. This is a heuristic method to allow us to be more confident of our model's prediction on general undiscovered data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we split up the data set into k parts, where k is an integer. Then we train the model on k-1 parts of the data set and test on the remaining. We then iterate the training process with different sets of k-1 parts of the data and test on the remaining. We then calculate the average MSE from the process and call it the **k-fold cross validation error**. This can be used to determine model suitability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, we have a cross_validate object that will allow us to do cross validation easily. However, you need to be aware of the tendency of the cross_validation process to validate on a metric of its own choosing. This is why when we are comparing models from cross validation, we need to ensure that the scoring parameter is standard across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cv_results = cross_validate(ridge_model,x,y,cv= 5,return_train_score=True,scoring='neg_mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the average mean squared error from the cross validation process, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.08057033148315294"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In general, the higher the cross validated negative mean squared error, the better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we have many model parameters to consider? (recall the $\\alpha$ parameter for the LASSO and Ridge models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through a list of $\\alpha$ parameters and obtaining a CV error on each parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [0.1, 1, 3, 5, 10, 100, 101]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'alpha': [0.1,1,3,5,10,100,101]\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "clf = GridSearchCV(Ridge(),parameters,scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can get the parameter with which the mean test score (the CV error) is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_test_score = clf.cv_results_['mean_test_score']\n",
    "np.argmax(mean_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07687840665701587"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_test_score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['alpha'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we see that AMONGST the alpha parameters we tested on, $\\alpha = 1$ gives the best CV error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can see how the parameters compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>-0.097619</td>\n",
       "      <td>-0.066233</td>\n",
       "      <td>-0.061804</td>\n",
       "      <td>-0.075219</td>\n",
       "      <td>0.015942</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.056805</td>\n",
       "      <td>-0.069420</td>\n",
       "      <td>-0.073229</td>\n",
       "      <td>-0.066485</td>\n",
       "      <td>0.007019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>1</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>-0.102200</td>\n",
       "      <td>-0.067595</td>\n",
       "      <td>-0.060841</td>\n",
       "      <td>-0.076878</td>\n",
       "      <td>0.018116</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.057733</td>\n",
       "      <td>-0.070807</td>\n",
       "      <td>-0.075454</td>\n",
       "      <td>-0.067998</td>\n",
       "      <td>0.007502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>3</td>\n",
       "      <td>{'alpha': 3}</td>\n",
       "      <td>-0.109258</td>\n",
       "      <td>-0.071547</td>\n",
       "      <td>-0.066866</td>\n",
       "      <td>-0.082557</td>\n",
       "      <td>0.018977</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.060790</td>\n",
       "      <td>-0.074786</td>\n",
       "      <td>-0.080417</td>\n",
       "      <td>-0.071998</td>\n",
       "      <td>0.008252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>5</td>\n",
       "      <td>{'alpha': 5}</td>\n",
       "      <td>-0.114186</td>\n",
       "      <td>-0.074741</td>\n",
       "      <td>-0.072572</td>\n",
       "      <td>-0.087167</td>\n",
       "      <td>0.019126</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.063563</td>\n",
       "      <td>-0.078055</td>\n",
       "      <td>-0.083929</td>\n",
       "      <td>-0.075182</td>\n",
       "      <td>0.008559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>10</td>\n",
       "      <td>{'alpha': 10}</td>\n",
       "      <td>-0.122868</td>\n",
       "      <td>-0.080856</td>\n",
       "      <td>-0.083714</td>\n",
       "      <td>-0.095812</td>\n",
       "      <td>0.019167</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.069260</td>\n",
       "      <td>-0.084421</td>\n",
       "      <td>-0.090069</td>\n",
       "      <td>-0.081250</td>\n",
       "      <td>0.008786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0       0.001081      0.000539         0.000219        0.000033         0.1   \n",
       "1       0.000614      0.000018         0.000195        0.000020           1   \n",
       "2       0.000698      0.000034         0.000203        0.000025           3   \n",
       "3       0.000601      0.000001         0.000207        0.000038           5   \n",
       "4       0.000789      0.000031         0.000206        0.000015          10   \n",
       "\n",
       "           params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'alpha': 0.1}          -0.097619          -0.066233          -0.061804   \n",
       "1    {'alpha': 1}          -0.102200          -0.067595          -0.060841   \n",
       "2    {'alpha': 3}          -0.109258          -0.071547          -0.066866   \n",
       "3    {'alpha': 5}          -0.114186          -0.074741          -0.072572   \n",
       "4   {'alpha': 10}          -0.122868          -0.080856          -0.083714   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0        -0.075219        0.015942                1           -0.056805   \n",
       "1        -0.076878        0.018116                2           -0.057733   \n",
       "2        -0.082557        0.018977                3           -0.060790   \n",
       "3        -0.087167        0.019126                4           -0.063563   \n",
       "4        -0.095812        0.019167                5           -0.069260   \n",
       "\n",
       "   split1_train_score  split2_train_score  mean_train_score  std_train_score  \n",
       "0           -0.069420           -0.073229         -0.066485         0.007019  \n",
       "1           -0.070807           -0.075454         -0.067998         0.007502  \n",
       "2           -0.074786           -0.080417         -0.071998         0.008252  \n",
       "3           -0.078055           -0.083929         -0.075182         0.008559  \n",
       "4           -0.084421           -0.090069         -0.081250         0.008786  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(clf.cv_results_)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we use the kNN regressor to predict the sepal width now? We need to first import the classifier from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "          metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "          weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [1, 2, 3, 4, 5, 10, 50], 'weights': ['uniform', 'distance', <function <lambda> at 0x1154ff1e0>, <function <lambda> at 0x1154ff268>]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'n_neighbors': [1,2,3,4,5,10,50],\n",
    "    'weights' : ['uniform','distance',lambda x: [1/(1+i) for i in x],lambda x: [1/(2+i) for i in x]]\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "kNNclf = GridSearchCV(KNeighborsRegressor(),parameters,scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "kNNclf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(kNNclf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_neighbors</th>\n",
       "      <th>param_weights</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 1, 'weights': 'uniform'}</td>\n",
       "      <td>-0.142286</td>\n",
       "      <td>-0.125429</td>\n",
       "      <td>-0.140857</td>\n",
       "      <td>-0.136190</td>\n",
       "      <td>0.007632</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.001857</td>\n",
       "      <td>-0.000810</td>\n",
       "      <td>0.000777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'n_neighbors': 1, 'weights': 'distance'}</td>\n",
       "      <td>-0.142286</td>\n",
       "      <td>-0.125429</td>\n",
       "      <td>-0.140857</td>\n",
       "      <td>-0.136190</td>\n",
       "      <td>0.007632</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.001857</td>\n",
       "      <td>-0.000810</td>\n",
       "      <td>0.000777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;function &lt;lambda&gt; at 0x1154ff1e0&gt;</td>\n",
       "      <td>{'n_neighbors': 1, 'weights': &lt;function &lt;lambd...</td>\n",
       "      <td>-0.142286</td>\n",
       "      <td>-0.125429</td>\n",
       "      <td>-0.140857</td>\n",
       "      <td>-0.136190</td>\n",
       "      <td>0.007632</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.001857</td>\n",
       "      <td>-0.000810</td>\n",
       "      <td>0.000777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;function &lt;lambda&gt; at 0x1154ff268&gt;</td>\n",
       "      <td>{'n_neighbors': 1, 'weights': &lt;function &lt;lambd...</td>\n",
       "      <td>-0.142286</td>\n",
       "      <td>-0.125429</td>\n",
       "      <td>-0.140857</td>\n",
       "      <td>-0.136190</td>\n",
       "      <td>0.007632</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.001857</td>\n",
       "      <td>-0.000810</td>\n",
       "      <td>0.000777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 2, 'weights': 'uniform'}</td>\n",
       "      <td>-0.155071</td>\n",
       "      <td>-0.080500</td>\n",
       "      <td>-0.106286</td>\n",
       "      <td>-0.113952</td>\n",
       "      <td>0.030923</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.023000</td>\n",
       "      <td>-0.036500</td>\n",
       "      <td>-0.037107</td>\n",
       "      <td>-0.032202</td>\n",
       "      <td>0.006512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'n_neighbors': 2, 'weights': 'distance'}</td>\n",
       "      <td>-0.144789</td>\n",
       "      <td>-0.085623</td>\n",
       "      <td>-0.108642</td>\n",
       "      <td>-0.113018</td>\n",
       "      <td>0.024352</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000929</td>\n",
       "      <td>-0.000405</td>\n",
       "      <td>0.000388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;function &lt;lambda&gt; at 0x1154ff1e0&gt;</td>\n",
       "      <td>{'n_neighbors': 2, 'weights': &lt;function &lt;lambd...</td>\n",
       "      <td>-0.153559</td>\n",
       "      <td>-0.081013</td>\n",
       "      <td>-0.105471</td>\n",
       "      <td>-0.113348</td>\n",
       "      <td>0.030136</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.018812</td>\n",
       "      <td>-0.030375</td>\n",
       "      <td>-0.030177</td>\n",
       "      <td>-0.026455</td>\n",
       "      <td>0.005405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;function &lt;lambda&gt; at 0x1154ff268&gt;</td>\n",
       "      <td>{'n_neighbors': 2, 'weights': &lt;function &lt;lambd...</td>\n",
       "      <td>-0.154263</td>\n",
       "      <td>-0.080773</td>\n",
       "      <td>-0.105709</td>\n",
       "      <td>-0.113582</td>\n",
       "      <td>0.030514</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.020677</td>\n",
       "      <td>-0.033186</td>\n",
       "      <td>-0.033314</td>\n",
       "      <td>-0.029059</td>\n",
       "      <td>0.005927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 3, 'weights': 'uniform'}</td>\n",
       "      <td>-0.141683</td>\n",
       "      <td>-0.086063</td>\n",
       "      <td>-0.096349</td>\n",
       "      <td>-0.108032</td>\n",
       "      <td>0.024162</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.036190</td>\n",
       "      <td>-0.049000</td>\n",
       "      <td>-0.049683</td>\n",
       "      <td>-0.044958</td>\n",
       "      <td>0.006206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'n_neighbors': 3, 'weights': 'distance'}</td>\n",
       "      <td>-0.134702</td>\n",
       "      <td>-0.090303</td>\n",
       "      <td>-0.101084</td>\n",
       "      <td>-0.108696</td>\n",
       "      <td>0.018908</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000929</td>\n",
       "      <td>-0.000405</td>\n",
       "      <td>0.000388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;function &lt;lambda&gt; at 0x1154ff1e0&gt;</td>\n",
       "      <td>{'n_neighbors': 3, 'weights': &lt;function &lt;lambd...</td>\n",
       "      <td>-0.141040</td>\n",
       "      <td>-0.086261</td>\n",
       "      <td>-0.095695</td>\n",
       "      <td>-0.107666</td>\n",
       "      <td>0.023912</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.031081</td>\n",
       "      <td>-0.041262</td>\n",
       "      <td>-0.042565</td>\n",
       "      <td>-0.038303</td>\n",
       "      <td>0.005134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;function &lt;lambda&gt; at 0x1154ff268&gt;</td>\n",
       "      <td>{'n_neighbors': 3, 'weights': &lt;function &lt;lambd...</td>\n",
       "      <td>-0.141378</td>\n",
       "      <td>-0.086137</td>\n",
       "      <td>-0.095867</td>\n",
       "      <td>-0.107794</td>\n",
       "      <td>0.024077</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.033437</td>\n",
       "      <td>-0.044721</td>\n",
       "      <td>-0.045876</td>\n",
       "      <td>-0.041345</td>\n",
       "      <td>0.005611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>4</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 4, 'weights': 'uniform'}</td>\n",
       "      <td>-0.132071</td>\n",
       "      <td>-0.075893</td>\n",
       "      <td>-0.089554</td>\n",
       "      <td>-0.099173</td>\n",
       "      <td>0.023922</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.044241</td>\n",
       "      <td>-0.060375</td>\n",
       "      <td>-0.058768</td>\n",
       "      <td>-0.054461</td>\n",
       "      <td>0.007257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>4</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'n_neighbors': 4, 'weights': 'distance'}</td>\n",
       "      <td>-0.127859</td>\n",
       "      <td>-0.080360</td>\n",
       "      <td>-0.092870</td>\n",
       "      <td>-0.100363</td>\n",
       "      <td>0.020102</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000929</td>\n",
       "      <td>-0.000405</td>\n",
       "      <td>0.000388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;function &lt;lambda&gt; at 0x1154ff1e0&gt;</td>\n",
       "      <td>{'n_neighbors': 4, 'weights': &lt;function &lt;lambd...</td>\n",
       "      <td>-0.132291</td>\n",
       "      <td>-0.075509</td>\n",
       "      <td>-0.089093</td>\n",
       "      <td>-0.098964</td>\n",
       "      <td>0.024209</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.038233</td>\n",
       "      <td>-0.051288</td>\n",
       "      <td>-0.050736</td>\n",
       "      <td>-0.046753</td>\n",
       "      <td>0.006028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;function &lt;lambda&gt; at 0x1154ff268&gt;</td>\n",
       "      <td>{'n_neighbors': 4, 'weights': &lt;function &lt;lambd...</td>\n",
       "      <td>-0.132320</td>\n",
       "      <td>-0.075565</td>\n",
       "      <td>-0.089231</td>\n",
       "      <td>-0.099039</td>\n",
       "      <td>0.024186</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.040968</td>\n",
       "      <td>-0.055346</td>\n",
       "      <td>-0.054429</td>\n",
       "      <td>-0.050248</td>\n",
       "      <td>0.006572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 5, 'weights': 'uniform'}</td>\n",
       "      <td>-0.122514</td>\n",
       "      <td>-0.067063</td>\n",
       "      <td>-0.082194</td>\n",
       "      <td>-0.090590</td>\n",
       "      <td>0.023404</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.045703</td>\n",
       "      <td>-0.061069</td>\n",
       "      <td>-0.063531</td>\n",
       "      <td>-0.056768</td>\n",
       "      <td>0.007888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'n_neighbors': 5, 'weights': 'distance'}</td>\n",
       "      <td>-0.122379</td>\n",
       "      <td>-0.073459</td>\n",
       "      <td>-0.085059</td>\n",
       "      <td>-0.093632</td>\n",
       "      <td>0.020871</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000929</td>\n",
       "      <td>-0.000405</td>\n",
       "      <td>0.000388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;function &lt;lambda&gt; at 0x1154ff1e0&gt;</td>\n",
       "      <td>{'n_neighbors': 5, 'weights': &lt;function &lt;lambd...</td>\n",
       "      <td>-0.123428</td>\n",
       "      <td>-0.067399</td>\n",
       "      <td>-0.081740</td>\n",
       "      <td>-0.090856</td>\n",
       "      <td>0.023764</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.040197</td>\n",
       "      <td>-0.052980</td>\n",
       "      <td>-0.055213</td>\n",
       "      <td>-0.049463</td>\n",
       "      <td>0.006615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;function &lt;lambda&gt; at 0x1154ff268&gt;</td>\n",
       "      <td>{'n_neighbors': 5, 'weights': &lt;function &lt;lambd...</td>\n",
       "      <td>-0.123158</td>\n",
       "      <td>-0.067201</td>\n",
       "      <td>-0.081890</td>\n",
       "      <td>-0.090750</td>\n",
       "      <td>0.023688</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.042727</td>\n",
       "      <td>-0.056656</td>\n",
       "      <td>-0.059011</td>\n",
       "      <td>-0.052798</td>\n",
       "      <td>0.007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>10</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 10, 'weights': 'uniform'}</td>\n",
       "      <td>-0.116389</td>\n",
       "      <td>-0.072191</td>\n",
       "      <td>-0.080594</td>\n",
       "      <td>-0.089725</td>\n",
       "      <td>0.019164</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.053057</td>\n",
       "      <td>-0.074914</td>\n",
       "      <td>-0.080354</td>\n",
       "      <td>-0.069442</td>\n",
       "      <td>0.011797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>10</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'n_neighbors': 10, 'weights': 'distance'}</td>\n",
       "      <td>-0.116805</td>\n",
       "      <td>-0.071308</td>\n",
       "      <td>-0.073277</td>\n",
       "      <td>-0.087130</td>\n",
       "      <td>0.020999</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000929</td>\n",
       "      <td>-0.000405</td>\n",
       "      <td>0.000388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>10</td>\n",
       "      <td>&lt;function &lt;lambda&gt; at 0x1154ff1e0&gt;</td>\n",
       "      <td>{'n_neighbors': 10, 'weights': &lt;function &lt;lamb...</td>\n",
       "      <td>-0.116468</td>\n",
       "      <td>-0.070121</td>\n",
       "      <td>-0.077815</td>\n",
       "      <td>-0.088135</td>\n",
       "      <td>0.020279</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.047626</td>\n",
       "      <td>-0.066879</td>\n",
       "      <td>-0.071585</td>\n",
       "      <td>-0.062030</td>\n",
       "      <td>0.010365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>10</td>\n",
       "      <td>&lt;function &lt;lambda&gt; at 0x1154ff268&gt;</td>\n",
       "      <td>{'n_neighbors': 10, 'weights': &lt;function &lt;lamb...</td>\n",
       "      <td>-0.116462</td>\n",
       "      <td>-0.070779</td>\n",
       "      <td>-0.078933</td>\n",
       "      <td>-0.088725</td>\n",
       "      <td>0.019894</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.050061</td>\n",
       "      <td>-0.070528</td>\n",
       "      <td>-0.075516</td>\n",
       "      <td>-0.065368</td>\n",
       "      <td>0.011014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>50</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 50, 'weights': 'uniform'}</td>\n",
       "      <td>-0.276418</td>\n",
       "      <td>-0.180363</td>\n",
       "      <td>-0.217739</td>\n",
       "      <td>-0.224840</td>\n",
       "      <td>0.039534</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.178791</td>\n",
       "      <td>-0.180430</td>\n",
       "      <td>-0.162310</td>\n",
       "      <td>-0.173843</td>\n",
       "      <td>0.008183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>50</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'n_neighbors': 50, 'weights': 'distance'}</td>\n",
       "      <td>-0.151460</td>\n",
       "      <td>-0.100081</td>\n",
       "      <td>-0.101265</td>\n",
       "      <td>-0.117602</td>\n",
       "      <td>0.023946</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000929</td>\n",
       "      <td>-0.000405</td>\n",
       "      <td>0.000388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;function &lt;lambda&gt; at 0x1154ff1e0&gt;</td>\n",
       "      <td>{'n_neighbors': 50, 'weights': &lt;function &lt;lamb...</td>\n",
       "      <td>-0.189078</td>\n",
       "      <td>-0.133085</td>\n",
       "      <td>-0.148026</td>\n",
       "      <td>-0.156730</td>\n",
       "      <td>0.023673</td>\n",
       "      <td>26</td>\n",
       "      <td>-0.106371</td>\n",
       "      <td>-0.125315</td>\n",
       "      <td>-0.120131</td>\n",
       "      <td>-0.117272</td>\n",
       "      <td>0.007994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;function &lt;lambda&gt; at 0x1154ff268&gt;</td>\n",
       "      <td>{'n_neighbors': 50, 'weights': &lt;function &lt;lamb...</td>\n",
       "      <td>-0.208512</td>\n",
       "      <td>-0.144531</td>\n",
       "      <td>-0.164673</td>\n",
       "      <td>-0.172572</td>\n",
       "      <td>0.026711</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.124182</td>\n",
       "      <td>-0.138898</td>\n",
       "      <td>-0.130807</td>\n",
       "      <td>-0.131296</td>\n",
       "      <td>0.006018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.000639      0.000073         0.000733        0.000189   \n",
       "1        0.000387      0.000009         0.000478        0.000008   \n",
       "2        0.000556      0.000106         0.000644        0.000034   \n",
       "3        0.000443      0.000048         0.000628        0.000027   \n",
       "4        0.000380      0.000009         0.000426        0.000002   \n",
       "5        0.000379      0.000004         0.000473        0.000006   \n",
       "6        0.000429      0.000006         0.000611        0.000008   \n",
       "7        0.000575      0.000155         0.000790        0.000179   \n",
       "8        0.000590      0.000103         0.000650        0.000080   \n",
       "9        0.000471      0.000029         0.000542        0.000039   \n",
       "10       0.000460      0.000034         0.000657        0.000041   \n",
       "11       0.000489      0.000121         0.000634        0.000035   \n",
       "12       0.000385      0.000006         0.000440        0.000005   \n",
       "13       0.000441      0.000072         0.000515        0.000029   \n",
       "14       0.000465      0.000064         0.000652        0.000032   \n",
       "15       0.000471      0.000019         0.000661        0.000012   \n",
       "16       0.000478      0.000012         0.000500        0.000005   \n",
       "17       0.000435      0.000011         0.000531        0.000009   \n",
       "18       0.000566      0.000157         0.000839        0.000108   \n",
       "19       0.000528      0.000083         0.000687        0.000044   \n",
       "20       0.000502      0.000041         0.000610        0.000088   \n",
       "21       0.000515      0.000111         0.000624        0.000074   \n",
       "22       0.000375      0.000003         0.000621        0.000007   \n",
       "23       0.000516      0.000079         0.000749        0.000084   \n",
       "24       0.000306      0.000005         0.000473        0.000020   \n",
       "25       0.000324      0.000018         0.000517        0.000002   \n",
       "26       0.000306      0.000028         0.000661        0.000037   \n",
       "27       0.000431      0.000180         0.000771        0.000160   \n",
       "\n",
       "   param_n_neighbors                       param_weights  \\\n",
       "0                  1                             uniform   \n",
       "1                  1                            distance   \n",
       "2                  1  <function <lambda> at 0x1154ff1e0>   \n",
       "3                  1  <function <lambda> at 0x1154ff268>   \n",
       "4                  2                             uniform   \n",
       "5                  2                            distance   \n",
       "6                  2  <function <lambda> at 0x1154ff1e0>   \n",
       "7                  2  <function <lambda> at 0x1154ff268>   \n",
       "8                  3                             uniform   \n",
       "9                  3                            distance   \n",
       "10                 3  <function <lambda> at 0x1154ff1e0>   \n",
       "11                 3  <function <lambda> at 0x1154ff268>   \n",
       "12                 4                             uniform   \n",
       "13                 4                            distance   \n",
       "14                 4  <function <lambda> at 0x1154ff1e0>   \n",
       "15                 4  <function <lambda> at 0x1154ff268>   \n",
       "16                 5                             uniform   \n",
       "17                 5                            distance   \n",
       "18                 5  <function <lambda> at 0x1154ff1e0>   \n",
       "19                 5  <function <lambda> at 0x1154ff268>   \n",
       "20                10                             uniform   \n",
       "21                10                            distance   \n",
       "22                10  <function <lambda> at 0x1154ff1e0>   \n",
       "23                10  <function <lambda> at 0x1154ff268>   \n",
       "24                50                             uniform   \n",
       "25                50                            distance   \n",
       "26                50  <function <lambda> at 0x1154ff1e0>   \n",
       "27                50  <function <lambda> at 0x1154ff268>   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0            {'n_neighbors': 1, 'weights': 'uniform'}          -0.142286   \n",
       "1           {'n_neighbors': 1, 'weights': 'distance'}          -0.142286   \n",
       "2   {'n_neighbors': 1, 'weights': <function <lambd...          -0.142286   \n",
       "3   {'n_neighbors': 1, 'weights': <function <lambd...          -0.142286   \n",
       "4            {'n_neighbors': 2, 'weights': 'uniform'}          -0.155071   \n",
       "5           {'n_neighbors': 2, 'weights': 'distance'}          -0.144789   \n",
       "6   {'n_neighbors': 2, 'weights': <function <lambd...          -0.153559   \n",
       "7   {'n_neighbors': 2, 'weights': <function <lambd...          -0.154263   \n",
       "8            {'n_neighbors': 3, 'weights': 'uniform'}          -0.141683   \n",
       "9           {'n_neighbors': 3, 'weights': 'distance'}          -0.134702   \n",
       "10  {'n_neighbors': 3, 'weights': <function <lambd...          -0.141040   \n",
       "11  {'n_neighbors': 3, 'weights': <function <lambd...          -0.141378   \n",
       "12           {'n_neighbors': 4, 'weights': 'uniform'}          -0.132071   \n",
       "13          {'n_neighbors': 4, 'weights': 'distance'}          -0.127859   \n",
       "14  {'n_neighbors': 4, 'weights': <function <lambd...          -0.132291   \n",
       "15  {'n_neighbors': 4, 'weights': <function <lambd...          -0.132320   \n",
       "16           {'n_neighbors': 5, 'weights': 'uniform'}          -0.122514   \n",
       "17          {'n_neighbors': 5, 'weights': 'distance'}          -0.122379   \n",
       "18  {'n_neighbors': 5, 'weights': <function <lambd...          -0.123428   \n",
       "19  {'n_neighbors': 5, 'weights': <function <lambd...          -0.123158   \n",
       "20          {'n_neighbors': 10, 'weights': 'uniform'}          -0.116389   \n",
       "21         {'n_neighbors': 10, 'weights': 'distance'}          -0.116805   \n",
       "22  {'n_neighbors': 10, 'weights': <function <lamb...          -0.116468   \n",
       "23  {'n_neighbors': 10, 'weights': <function <lamb...          -0.116462   \n",
       "24          {'n_neighbors': 50, 'weights': 'uniform'}          -0.276418   \n",
       "25         {'n_neighbors': 50, 'weights': 'distance'}          -0.151460   \n",
       "26  {'n_neighbors': 50, 'weights': <function <lamb...          -0.189078   \n",
       "27  {'n_neighbors': 50, 'weights': <function <lamb...          -0.208512   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           -0.125429          -0.140857        -0.136190        0.007632   \n",
       "1           -0.125429          -0.140857        -0.136190        0.007632   \n",
       "2           -0.125429          -0.140857        -0.136190        0.007632   \n",
       "3           -0.125429          -0.140857        -0.136190        0.007632   \n",
       "4           -0.080500          -0.106286        -0.113952        0.030923   \n",
       "5           -0.085623          -0.108642        -0.113018        0.024352   \n",
       "6           -0.081013          -0.105471        -0.113348        0.030136   \n",
       "7           -0.080773          -0.105709        -0.113582        0.030514   \n",
       "8           -0.086063          -0.096349        -0.108032        0.024162   \n",
       "9           -0.090303          -0.101084        -0.108696        0.018908   \n",
       "10          -0.086261          -0.095695        -0.107666        0.023912   \n",
       "11          -0.086137          -0.095867        -0.107794        0.024077   \n",
       "12          -0.075893          -0.089554        -0.099173        0.023922   \n",
       "13          -0.080360          -0.092870        -0.100363        0.020102   \n",
       "14          -0.075509          -0.089093        -0.098964        0.024209   \n",
       "15          -0.075565          -0.089231        -0.099039        0.024186   \n",
       "16          -0.067063          -0.082194        -0.090590        0.023404   \n",
       "17          -0.073459          -0.085059        -0.093632        0.020871   \n",
       "18          -0.067399          -0.081740        -0.090856        0.023764   \n",
       "19          -0.067201          -0.081890        -0.090750        0.023688   \n",
       "20          -0.072191          -0.080594        -0.089725        0.019164   \n",
       "21          -0.071308          -0.073277        -0.087130        0.020999   \n",
       "22          -0.070121          -0.077815        -0.088135        0.020279   \n",
       "23          -0.070779          -0.078933        -0.088725        0.019894   \n",
       "24          -0.180363          -0.217739        -0.224840        0.039534   \n",
       "25          -0.100081          -0.101265        -0.117602        0.023946   \n",
       "26          -0.133085          -0.148026        -0.156730        0.023673   \n",
       "27          -0.144531          -0.164673        -0.172572        0.026711   \n",
       "\n",
       "    rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                23           -0.000571           -0.000000   \n",
       "1                22           -0.000571           -0.000000   \n",
       "2                23           -0.000571           -0.000000   \n",
       "3                23           -0.000571           -0.000000   \n",
       "4                20           -0.023000           -0.036500   \n",
       "5                17           -0.000286           -0.000000   \n",
       "6                18           -0.018812           -0.030375   \n",
       "7                19           -0.020677           -0.033186   \n",
       "8                15           -0.036190           -0.049000   \n",
       "9                16           -0.000286           -0.000000   \n",
       "10               13           -0.031081           -0.041262   \n",
       "11               14           -0.033437           -0.044721   \n",
       "12               11           -0.044241           -0.060375   \n",
       "13               12           -0.000286           -0.000000   \n",
       "14                9           -0.038233           -0.051288   \n",
       "15               10           -0.040968           -0.055346   \n",
       "16                5           -0.045703           -0.061069   \n",
       "17                8           -0.000286           -0.000000   \n",
       "18                7           -0.040197           -0.052980   \n",
       "19                6           -0.042727           -0.056656   \n",
       "20                4           -0.053057           -0.074914   \n",
       "21                1           -0.000286           -0.000000   \n",
       "22                2           -0.047626           -0.066879   \n",
       "23                3           -0.050061           -0.070528   \n",
       "24               28           -0.178791           -0.180430   \n",
       "25               21           -0.000286           -0.000000   \n",
       "26               26           -0.106371           -0.125315   \n",
       "27               27           -0.124182           -0.138898   \n",
       "\n",
       "    split2_train_score  mean_train_score  std_train_score  \n",
       "0            -0.001857         -0.000810         0.000777  \n",
       "1            -0.001857         -0.000810         0.000777  \n",
       "2            -0.001857         -0.000810         0.000777  \n",
       "3            -0.001857         -0.000810         0.000777  \n",
       "4            -0.037107         -0.032202         0.006512  \n",
       "5            -0.000929         -0.000405         0.000388  \n",
       "6            -0.030177         -0.026455         0.005405  \n",
       "7            -0.033314         -0.029059         0.005927  \n",
       "8            -0.049683         -0.044958         0.006206  \n",
       "9            -0.000929         -0.000405         0.000388  \n",
       "10           -0.042565         -0.038303         0.005134  \n",
       "11           -0.045876         -0.041345         0.005611  \n",
       "12           -0.058768         -0.054461         0.007257  \n",
       "13           -0.000929         -0.000405         0.000388  \n",
       "14           -0.050736         -0.046753         0.006028  \n",
       "15           -0.054429         -0.050248         0.006572  \n",
       "16           -0.063531         -0.056768         0.007888  \n",
       "17           -0.000929         -0.000405         0.000388  \n",
       "18           -0.055213         -0.049463         0.006615  \n",
       "19           -0.059011         -0.052798         0.007186  \n",
       "20           -0.080354         -0.069442         0.011797  \n",
       "21           -0.000929         -0.000405         0.000388  \n",
       "22           -0.071585         -0.062030         0.010365  \n",
       "23           -0.075516         -0.065368         0.011014  \n",
       "24           -0.162310         -0.173843         0.008183  \n",
       "25           -0.000929         -0.000405         0.000388  \n",
       "26           -0.120131         -0.117272         0.007994  \n",
       "27           -0.130807         -0.131296         0.006018  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
